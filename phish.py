# -*- coding: utf-8 -*-
"""phish.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QAmaGeDOivraa9qdWlNXSBhxuXvdnuE8
"""

!pip install torch torch-geometric scikit-learn pandas networkx

import numpy
print(numpy.__version__)

import torch
print(torch.__version__)

# Torch-scatter and torch-sparse compatible with torch==2.6.0+cu124
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html -q
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html -q

import pandas as pd
import torch
from torch_geometric.data import Data
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder, StandardScaler
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# === Load Data ===
df = pd.read_excel("/content/drive/MyDrive/phish/phish.xlsx")

# === Drop identifier/date columns ===
drop_cols = ['IP', 'Domain', 'Domain_Name', 'Creation_Date_Time', 'Registrant_Name']
df = df.drop(columns=drop_cols, errors='ignore')

# === Separate categorical and numeric columns ===
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# === Handle categorical columns: convert to string and encode ===
df[categorical_cols] = df[categorical_cols].astype(str).fillna("missing")
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

# === Combine updated columns ===
all_cols = categorical_cols + numeric_cols
df = df[all_cols].fillna(0)

# === Use entropy as label (binary classification) ===
labels = (df['entropy'] > df['entropy'].median()).astype(int).values

# === Scale all features (excluding label) ===
features = df.drop(columns=['entropy']).values
scaler = StandardScaler()
features = scaler.fit_transform(features)

# === Graph Generation Parameters ===
window_size = 10
num_windows = len(df) // window_size
k = 5  # number of neighbors

data_list = []

# === Save all windows to a single Excel file ===
with pd.ExcelWriter('/content/drive/MyDrive/phish/phish_graph_windows.xlsx') as writer:
    for window in range(num_windows):
        start_idx = window * window_size
        end_idx = (window + 1) * window_size

        window_features = features[start_idx:end_idx]
        window_labels = labels[start_idx:end_idx]

        # Similarity and edge generation
        sim_matrix_window = cosine_similarity(window_features)
        edge_index_window = []
        for i in range(len(sim_matrix_window)):
            top_k = np.argsort(sim_matrix_window[i])[-(k+1):-1]  # skip self-loop
            for j in top_k:
                edge_index_window.append([i, j])

        edge_index_window = torch.tensor(edge_index_window, dtype=torch.long).t().contiguous()
        x_window = torch.tensor(window_features, dtype=torch.float)
        y_window = torch.tensor(window_labels, dtype=torch.long)

        data_window = Data(x=x_window, edge_index=edge_index_window, y=y_window)
        data_list.append(data_window)

        # Save to Excel
        pd.DataFrame(x_window.numpy()).to_excel(writer, sheet_name=f'Window_{window}_Features', index=False)
        pd.DataFrame(edge_index_window.numpy().T, columns=["Source", "Target"]).to_excel(writer, sheet_name=f'Window_{window}_Edges', index=False)
        pd.DataFrame(y_window.numpy(), columns=["Labels"]).to_excel(writer, sheet_name=f'Window_{window}_Labels', index=False)

""" Step 1: Dynamic Graph Learning with GraphSAGE"""

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

# Define GraphSAGE Model
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

""" Train over multiple windows"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model params
in_channels = data_list[0].num_node_features
hidden_channels = 32
out_channels = 2

# Model, optimizer, loss
model = GraphSAGE(in_channels, hidden_channels, out_channels).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = torch.nn.CrossEntropyLoss()

# For plotting
loss_values = []
accuracy_values = []

# Training loop
model.train()
for epoch in range(20):
    total_loss = 0
    all_preds = []
    all_labels = []

    for data in data_list:
        data = data.to(device)
        optimizer.zero_grad()

        out = model(data.x, data.edge_index)
        loss = loss_fn(out, data.y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        preds = out.argmax(dim=1).cpu().numpy()
        labels = data.y.cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels)

    acc = accuracy_score(all_labels, all_preds)
    loss_values.append(total_loss)
    accuracy_values.append(acc)

    print(f"Epoch {epoch + 1:2d} | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}")

# === Final evaluation scores ===
precision = precision_score(all_labels, all_preds, average='binary')
recall = recall_score(all_labels, all_preds, average='binary')
f1 = f1_score(all_labels, all_preds, average='binary')

print("\n--- Final Evaluation ---")
print(f"Accuracy : {accuracy_score(all_labels, all_preds):.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1-score : {f1:.4f}")

# === Plotting ===
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(loss_values, marker='o')
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")

plt.subplot(1, 2, 2)
plt.plot(accuracy_values, marker='s', color='green')
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")

plt.tight_layout()
plt.show()

print(data.x.shape)
# Output: torch.Size([1000, 16]) -> feature_dim = 16

num_classes = len(torch.unique(data.y))
print(num_classes)

"""GraphSAGE is imported:"""

from torch_geometric.nn import GraphSAGE

"""Basic Accuracy"""

# Evaluate on all windows using CPU
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for data in data_list:
        data = data.to("cpu")  # Set device to CPU
        out = model(data.x, data.edge_index)
        pred = out.argmax(dim=1)
        correct += (pred == data.y).sum().item()
        total += data.num_nodes

print(f"Accuracy: {correct / total * 100:.2f}%")
import torch

# Mount Google Drive (if not already mounted)
from google.colab import drive
drive.mount('/content/drive')

# Path to save the PyG data object
save_path = '/content/drive/MyDrive/phish/phish_graph_data.pt'

# Save the PyG Data object
torch.save(data_list, save_path)

print(f"Saved graph_data to {save_path}")

"""Causal Sampling (Temporal Neighbor Selection)

Updated Training Loop with Causal Sampling:
"""

from torch_geometric.nn import GraphSAGE
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

losses = []
accuracies = []

# Assume binary classification: phishing (1) or not (0)
in_channels = data_list[0].num_node_features
hidden_channels = 32
out_channels = 2  # For phishing vs non-phishing

model = GraphSAGE(in_channels, hidden_channels, out_channels).to("cpu")
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = torch.nn.CrossEntropyLoss()

# Causal training loop
model.train()
for epoch in range(20):
    total_loss = 0
    for t in range(1, len(data_list)):  # Causal window: t-1 and t
        optimizer.zero_grad()

        past_data = data_list[t - 1].to("cpu")
        current_data = data_list[t].to("cpu")

        out_past = model(past_data.x, past_data.edge_index)
        loss_past = loss_fn(out_past, past_data.y)

        out_current = model(current_data.x, current_data.edge_index)
        loss_current = loss_fn(out_current, current_data.y)

        loss = loss_past + loss_current
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Evaluation after epoch
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for t in range(1, len(data_list)):
            data = data_list[t].to("cpu")
            out = model(data.x, data.edge_index)
            preds = out.argmax(dim=1)
            correct += (preds == data.y).sum().item()
            total += data.y.size(0)

    accuracy = correct / total if total > 0 else 0
    losses.append(total_loss)
    accuracies.append(accuracy * 100)

    print(f"Epoch {epoch+1:2d}, Causal Loss: {total_loss:.4f}, Accuracy: {accuracy*100:.2f}%")
    model.train()

# Plot loss and accuracy
epochs = list(range(1, 21))
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, losses, marker='o', color='blue')
plt.title('Causal Training - Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.plot(epochs, accuracies, marker='s', color='green')
plt.title('Causal Training - Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')

plt.tight_layout()
plt.show()

"""Evaluation After Causal Training:"""

# Evaluate only on the last window (simulate unseen future)
model.eval()
correct = 0
total = 0

with torch.no_grad():
    eval_data = data_list[-1].to("cpu")  # Last window
    out = model(eval_data.x, eval_data.edge_index)
    pred = out.argmax(dim=1)
    correct += (pred == eval_data.y).sum().item()
    total += eval_data.num_nodes

print(f"Causal Evaluation Accuracy (on last window): {correct / total * 100:.2f}%")

import copy
import random
import torch

# Phishing is typically binary: 0 = legitimate, 1 = phishing
num_output_classes = 2
out_channels = num_output_classes

def inject_noise(data_list, label_noise_ratio=0.1, feature_noise_ratio=0.1, num_output_classes=2):
    noisy_data_list = []

    for data in data_list:
        data_noisy = copy.deepcopy(data)

        # Reassign labels if only one class exists
        unique_labels = list(set(data_noisy.y.tolist()))
        if len(unique_labels) <= 1:
            data_noisy.y = torch.randint(0, num_output_classes, data_noisy.y.shape, dtype=torch.long)
            unique_labels = list(set(data_noisy.y.tolist()))

        num_nodes = data_noisy.y.shape[0]

        # Inject label noise
        num_label_noise = int(label_noise_ratio * num_nodes)
        noisy_label_indices = random.sample(range(num_nodes), num_label_noise)

        for idx in noisy_label_indices:
            original_label = data_noisy.y[idx].item()
            possible_labels = list(range(num_output_classes))
            if original_label in possible_labels:
                possible_labels.remove(original_label)
            if not possible_labels:
                continue
            new_label = random.choice(possible_labels)
            data_noisy.y[idx] = new_label

        # Inject feature noise
        num_feature_noise = int(feature_noise_ratio * num_nodes)
        noisy_feature_indices = random.sample(range(num_nodes), num_feature_noise)
        for idx in noisy_feature_indices:
            noise = torch.randn_like(data_noisy.x[idx]) * 0.1
            data_noisy.x[idx] += noise

        noisy_data_list.append(data_noisy)

    return noisy_data_list


# Apply noise to phishing graph data
noisy_data_list = inject_noise(data_list, label_noise_ratio=0.1, feature_noise_ratio=0.1)

# Add dummy time_window values (for causal modeling)
for i, data in enumerate(noisy_data_list):
    data.time_window = torch.randint(0, 5, (data.num_nodes,), dtype=torch.long)


import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class CausalGraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(CausalGraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, time_window):
        row, col = edge_index
        mask = time_window[row] <= time_window[col]
        filtered_edge_index = edge_index if mask.sum() == 0 else edge_index[:, mask]
        x = F.relu(self.conv1(x, filtered_edge_index))
        x = self.conv2(x, filtered_edge_index)
        return x


# Training setup
model = CausalGraphSAGE(in_channels, hidden_channels, out_channels).to("cpu")
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Causal Training Loop
model.train()
for epoch in range(20):
    total_loss = 0
    for t in range(1, len(noisy_data_list)):
        optimizer.zero_grad()
        past_data = noisy_data_list[t - 1].to("cpu")
        current_data = noisy_data_list[t].to("cpu")

        out_past = model(past_data.x, past_data.edge_index, past_data.time_window)
        loss_past = loss_fn(out_past, past_data.y)

        out_current = model(current_data.x, current_data.edge_index, current_data.time_window)
        loss_current = loss_fn(out_current, current_data.y)

        loss = loss_past + loss_current
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"[Phishing] Epoch {epoch+1}, Causal Loss: {total_loss:.4f}")

print("Example past phishing labels:", past_data.y)
print("Example current phishing labels:", current_data.y)

"""Inject 10–20% Noise for Robustness Testing

We’ll inject:

Label noise: randomly flip labels for 10–20% of nodes.

Feature noise: add small Gaussian noise to features for 10–20% of nodes.
"""

import copy
import random
import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data

# --- Number of classes: 0 = legitimate, 1 = phishing ---
num_output_classes = 2

def inject_noise(data_list, label_noise_ratio=0.1, feature_noise_ratio=0.1, num_output_classes=2):
    noisy_data_list = []
    for data in data_list:
        data_noisy = copy.deepcopy(data)
        num_nodes = data_noisy.y.shape[0]

        # Re-map labels to [0, num_output_classes - 1] if necessary
        if len(set(data_noisy.y.tolist())) <= 1 or max(data_noisy.y.tolist()) >= num_output_classes:
            print(f"Warning: Adjusting labels in data sample {data_list.index(data)} to fit {num_output_classes} classes.")
            data_noisy.y = torch.randint(0, num_output_classes, data_noisy.y.shape, dtype=torch.long)

        # Inject label noise
        num_label_noise = int(label_noise_ratio * num_nodes)
        noisy_label_indices = random.sample(range(num_nodes), num_label_noise)
        for idx in noisy_label_indices:
            original_label = data_noisy.y[idx].item()
            possible_labels = list(range(num_output_classes))
            if original_label in possible_labels:
                possible_labels.remove(original_label)
            if not possible_labels:
                continue
            new_label = random.choice(possible_labels)
            data_noisy.y[idx] = new_label

        # Inject feature noise
        num_feature_noise = int(feature_noise_ratio * num_nodes)
        noisy_feature_indices = random.sample(range(num_nodes), num_feature_noise)
        for idx in noisy_feature_indices:
            noise = torch.randn_like(data_noisy.x[idx]) * 0.6
            data_noisy.x[idx] += noise

        noisy_data_list.append(data_noisy)
    return noisy_data_list

# Simulated phishing data for demonstration (replace with real graph data)
data_list = [
    Data(x=torch.randn(10, 16), edge_index=torch.randint(0, 10, (2, 20)), y=torch.randint(0, 2, (10,))),
    Data(x=torch.randn(12, 16), edge_index=torch.randint(0, 12, (2, 25)), y=torch.randint(0, 2, (12,))),
]

# Infer input feature size
if not data_list:
    raise ValueError("data_list is empty. Please load your phishing dataset.")
in_channels = data_list[0].x.shape[1]
hidden_channels = 32
out_channels = num_output_classes

# Add time_window (simulated timestamps)
for data in data_list:
    data.time_window = torch.randint(0, 5, (data.num_nodes,), dtype=torch.long)

# Inject noise
noisy_data_list = inject_noise(data_list, label_noise_ratio=0.3, feature_noise_ratio=0.3, num_output_classes=num_output_classes)

# Add time_window after noise injection
for data in noisy_data_list:
    data.time_window = torch.randint(0, 5, (data.num_nodes,), dtype=torch.long)

# Define GraphSAGE with causal time filtering
class CausalGraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(CausalGraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, time_window):
        row, col = edge_index
        if len(time_window) > 0:
            mask = time_window[row] <= time_window[col]
            filtered_edge_index = edge_index[:, mask] if mask.sum() > 0 else edge_index
        else:
            filtered_edge_index = edge_index

        x = F.relu(self.conv1(x, filtered_edge_index))
        x = self.conv2(x, filtered_edge_index)
        return x

# Setup
model = CausalGraphSAGE(in_channels, hidden_channels, out_channels).to("cpu")
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-10)
loss_fn = torch.nn.CrossEntropyLoss()

# Training
model.train()
for epoch in range(20):
    total_loss = 0
    if len(noisy_data_list) < 2:
        print("Not enough samples for training.")
        break

    for t in range(1, len(noisy_data_list)):
        optimizer.zero_grad()
        past_data = noisy_data_list[t - 1]
        current_data = noisy_data_list[t]

        # Ensure correct label dtype
        past_data.y = past_data.y.long()
        current_data.y = current_data.y.long()

        if past_data.y.max() >= out_channels or past_data.y.min() < 0:
            print(f"Invalid past labels at epoch {epoch}, t {t}")
            continue
        if current_data.y.max() >= out_channels or current_data.y.min() < 0:
            print(f"Invalid current labels at epoch {epoch}, t {t}")
            continue

        out_past = model(past_data.x, past_data.edge_index, past_data.time_window)
        loss_past = loss_fn(out_past, past_data.y)

        out_current = model(current_data.x, current_data.edge_index, current_data.time_window)
        loss_current = loss_fn(out_current, current_data.y)

        loss = loss_past + loss_current
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"[PhishNoise] Epoch {epoch+1}, Loss: {total_loss:.4f}")

print("Final past labels:", past_data.y)
print("Final current labels:", current_data.y)

"""Modify GraphSAGE to Handle Causal Sampling:"""

print(data.x.shape)  # Example: torch.Size([num_nodes, num_features])

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, BatchNorm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

class PhishGraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super(PhishGraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.bn1 = BatchNorm(hidden_channels)
        self.dropout = torch.nn.Dropout(dropout)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, time_window):
        row, col = edge_index

        # Causal filtering: only edges from earlier time steps allowed (if temporal)
        mask = time_window[row] < time_window[col]
        edge_index = edge_index[:, mask]

        x = self.conv1(x, edge_index)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x  # logits, apply CrossEntropyLoss outside

# Example: add time_window if missing (needed for causal filtering)
for t, data in enumerate(data_list):  # rename to your phishing dataset list
    if not hasattr(data, 'time_window'):
        data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)

# Initialize model — adjust input/output dimensions accordingly
# For example: phishing features might be 55, classes 2 (phish/not phish)
model = PhishGraphSAGE(in_channels=16, hidden_channels=32, out_channels=2, dropout=0.5)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
loss_fn = torch.nn.CrossEntropyLoss()
loss_history = []  # To store loss for each epoch

model.train()
for epoch in range(20):
    total_loss = 0
    all_preds = []
    all_labels = []

    for t, data in enumerate(data_list):
        optimizer.zero_grad()

        out = model(data.x, data.edge_index, data.time_window)

        loss = loss_fn(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        preds = out.argmax(dim=1).cpu()
        labels = data.y.cpu()
        all_preds.append(preds)
        all_labels.append(labels)

    avg_loss = total_loss / len(data_list)
    loss_history.append(avg_loss)  # save loss per epoch

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    acc = accuracy_score(all_labels, all_preds)

    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}")

# Plot loss curve after training completes
plt.plot(loss_history)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.show()

model.train()
for epoch in range(20):
    total_loss = 0
    for t, data in enumerate(noisy_data_list):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.time_window)
        loss = loss_fn(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

model.eval()  # set model to evaluation mode

all_preds = []
all_labels = []

with torch.no_grad():
    for data in noisy_data_list:
        out = model(data.x, data.edge_index, data.time_window)
        preds = out.argmax(dim=1).cpu()
        labels = data.y.cpu()

        all_preds.extend(preds.tolist())
        all_labels.extend(labels.tolist())

acc = accuracy_score(all_labels, all_preds)
prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)

print(f"Final Model Metrics -> Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

"""Training Loop with Causal Training:

How to Verify:
After you inject noise and assign the time_window to each data object:

Experiment with Strict vs. Non-Strict Causality:
Non-Strict Causality (<=): Allows nodes to consider neighbors from the current window (i.e., nodes can influence each other within the same time window).

Strict Causality (<): Prevents nodes from considering neighbors from the same window (i.e., nodes can only influence past time windows).

Full Code for Training Loop with Causal Sampling:

Summary:
The time_window should be an attribute of each Data object.

When performing causal sampling during the training or forward pass, reference the time_window correctly using the data.time_window[row] format.

Correctly apply the mask based on your causality preference (<= for non-strict, < for strict).

1. Aggregation of Valid Neighbors:
We will aggregate features from valid neighbors based on the causal mask. For simplicity, let's assume you're summing the features of valid neighbors. If you want to experiment with other aggregation methods like average or max, you can easily modify this logic.

2. Update Node Representations:
After aggregating the valid neighbors, we combine the node's features with the aggregated features (as typically done in GraphSAGE).

3. Complete Forward Pass:
The forward pass will involve processing each graph in noisy_data_list, applying the aggregation logic, and passing the updated node representations through the model.

4. Training Loop:
We’ll set up the training loop using your loss function (CrossEntropyLoss) and optimizer (Adam). We'll update the model's weights and compute the loss after each batch.

5. Evaluation:
After training, we’ll evaluate the model on a test set, and you can experiment with different metrics based on your task.
"""

print(f"Shape of features in the first window: {noisy_data_list[0].x.shape}")

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt

class CausalGraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(CausalGraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, time_window):
        row, col = edge_index
        mask = time_window[row] < time_window[col]  # strict causal edges
        filtered_edge_index = edge_index[:, mask] if mask.sum() > 0 else edge_index

        x = F.relu(self.conv1(x, filtered_edge_index))
        x = self.conv2(x, filtered_edge_index)
        return x

# Add time_window to data if not present
for t, data in enumerate(noisy_data_list):
    if not hasattr(data, 'time_window'):
        data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)

# Fix input feature size based on your data
in_channels = noisy_data_list[0].x.shape[1]  # Automatically detect
hidden_channels = 32
out_channels = 2
learning_rate = 0.01
num_epochs = 20

# Model, optimizer, loss function
model = CausalGraphSAGE(in_channels, hidden_channels, out_channels)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = torch.nn.CrossEntropyLoss()

# Tracking
epoch_losses = []
epoch_accuracies = []
epoch_precisions = []
epoch_recalls = []
epoch_f1s = []

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []

    for t, data in enumerate(noisy_data_list):
        optimizer.zero_grad()

        out = model(data.x, data.edge_index, data.time_window)

        loss = loss_fn(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        preds = out.argmax(dim=1).cpu()
        labels = data.y.cpu()
        all_preds.extend(preds.tolist())
        all_labels.extend(labels.tolist())

    acc = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)

    epoch_losses.append(total_loss)
    epoch_accuracies.append(acc)
    epoch_precisions.append(precision)
    epoch_recalls.append(recall)
    epoch_f1s.append(f1)

    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f} | Acc: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")

# Plotting
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(range(1, num_epochs+1), epoch_losses, label='Loss', color='red')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Epochs')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(range(1, num_epochs+1), epoch_accuracies, label='Accuracy')
plt.plot(range(1, num_epochs+1), epoch_precisions, label='Precision')
plt.plot(range(1, num_epochs+1), epoch_recalls, label='Recall')
plt.plot(range(1, num_epochs+1), epoch_f1s, label='F1 Score')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.title('Metrics over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

# Save model
model_path = "/content/drive/MyDrive/phish/phish_casual_graphsage_model.pth"
torch.save(model.state_dict(), model_path)
print(f"Model saved to {model_path}")

import torch.nn.functional as F
from sklearn.metrics import roc_auc_score

# Set model to evaluation mode
model.eval()
all_probs = []
all_labels = []

with torch.no_grad():
    for data in noisy_data_list:
        out = model(data.x, data.edge_index, data.time_window)  # raw logits for phishing detection
        probs = F.softmax(out, dim=1)  # Convert logits to class probabilities
        all_probs.append(probs.cpu())  # Store phishing probability predictions
        all_labels.append(data.y.cpu())  # Store true phishing labels

# Concatenate predictions and labels
all_probs = torch.cat(all_probs, dim=0).numpy()
all_labels = torch.cat(all_labels, dim=0).numpy()

# Compute AUC-ROC for phishing class (class 1 = phishing)
auc = roc_auc_score(all_labels, all_probs[:, 1])
print(f"AUC-ROC for Phishing Detection: {auc:.4f}")

"""Plotting the ROC Curve"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Get false positive rate (fpr), true positive rate (tpr), thresholds
fpr, tpr, thresholds = roc_curve(all_labels, all_probs[:, 1])

# Calculate AUC (just to confirm)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')  # diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()